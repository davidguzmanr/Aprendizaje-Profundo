%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Beamer Presentation
% LaTeX Template
% Version 1.0 (10/11/12)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND THEMES
%----------------------------------------------------------------------------------------

\documentclass[10pt]{beamer}

\mode<presentation> {
	
	% The Beamer class comes with a number of default slide themes
	% which change the colors and layouts of slides. Below this is a list
	% of all the themes, uncomment each in turn to see what they look like.
	
	%\usetheme{default}
	%\usetheme{AnnArbor}
	%\usetheme{Antibes}
	%\usetheme{Bergen}
	%\usetheme{Berkeley}
	%\usetheme{Berlin}
	%\usetheme{Boadilla}
	%\usetheme{CambridgeUS}
	%\usetheme{Copenhagen}
	%\usetheme{Darmstadt}
	%\usetheme{Dresden}
	%\usetheme{Frankfurt}
	%\usetheme{Goettingen}
	%\usetheme{Hannover}
	%\usetheme{Ilmenau}
	%\usetheme{JuanLesPins}
	%\usetheme{Luebeck}
	\usetheme{Madrid}
	%\usetheme{Malmoe}
	%\usetheme{Marburg}
	%\usetheme{Montpellier}
	%\usetheme{PaloAlto}
	%\usetheme{Pittsburgh}
	%\usetheme{Rochester}
	%\usetheme{Singapore}
	%\usetheme{Szeged}
	%\usetheme{Warsaw}
	
	% As well as themes, the Beamer class has a number of color themes
	% for any slide theme. Uncomment each of these in turn to see how it
	% changes the colors of your current slide theme.
	
	%\usecolortheme{albatross}
	\usecolortheme{beaver}
	%\usecolortheme{beetle}
	%\usecolortheme{crane}
	%\usecolortheme{dolphin}
	%\usecolortheme{dove}
	%\usecolortheme{fly}
	%\usecolortheme{lily}
	%\usecolortheme{orchid}
	%\usecolortheme{rose}
	%\usecolortheme{seagull}
	%\usecolortheme{seahorse}
	%\usecolortheme{whale}
	%\usecolortheme{wolverine}
	
	\usefonttheme{serif}
	
	%\setbeamertemplate{footline} % To remove the footer line in all slides uncomment this line
	%\setbeamertemplate{footline}[page number] % To replace the footer line in all slides with a simple slide count uncomment this line
	
	%\setbeamertemplate{navigation symbols}{} % To remove the navigation symbols from the bottom of all slides uncomment this line
}
\usepackage{ragged2e}
\usepackage{lipsum}
\usepackage{amsmath}
\usepackage{gensymb}
\usepackage{textcomp}
\usepackage{braket}
\usepackage{lipsum}
\usepackage{bm}
\usepackage{graphicx, float} % Allows including images
\usepackage[centerlast]{subfigure}
\usepackage{wrapfig}
\usepackage{booktabs} % Allows the use of \toprule, \midrule and \bottomrule in tables
\usepackage{multicol}
\usepackage[font=scriptsize,labelfont=scriptsize,justification=centering]{caption}
\usepackage[spanish,es-tabla]{babel}
\usepackage[utf8]{inputenc}
\usepackage{csquotes}
\spanishdecimal{.}
\usepackage{gensymb}
\usepackage{textcomp}
\usepackage{amsmath}
\usepackage{nicefrac}

\usepackage{listings}
\usepackage{xcolor}

\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=black,
    filecolor=black,      
    urlcolor=blue,
    citecolor=black,
}
\urlstyle{same}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\definecolor{codeblue}{RGB}{0, 180,201}
\definecolor{codered}{RGB}{238,25,0}



\lstdefinestyle{mystyle}{
	backgroundcolor=\color{backcolour},   
	commentstyle=\color{codeblue},
	keywordstyle=\color{codegreen},
	%numberstyle=\tiny\color{codegray},
	stringstyle=\color{codepurple},
	basicstyle=\ttfamily\footnotesize,
	breakatwhitespace=false,         
	breaklines=true,                 
	captionpos=b,                    
	keepspaces=true,                 
	%numbers=left,                    
	%numbersep=5pt,                  
	showspaces=false,                
	showstringspaces=false,
	showtabs=false,                  
	tabsize=2,
	moredelim=**[s][\color{codered}]{"""}{"""}
}

\lstset{emph={%  
		USE, KEYSPACE, WITH%
	},emphstyle={\color{codegreen}}%
}%

\lstset{literate = {á}{{\'a}}1 {é}{{\'e}}1 {í}{{\'i}}1 {Í}{{\'I}}1 {ó}{{\'o}}1 {ú}{{\'u}}1 {ñ}{{\~n}}1 {°}{{\textdegree}}1 {means-mu}{{$\mu$}}1 {std-sigma}{{$\sigma$}}1}

\lstset{language=Python, style=mystyle}

\newcommand\blfootnote[1]{%
  \begingroup
  \renewcommand\thefootnote{}\footnote{#1}%
  \addtocounter{footnote}{-1}%
  \endgroup
}

\setbeamertemplate{caption}[numbered]
%\captionsetup[figure]{font=scriptsize,labelfont=footnotesize}
%\captionsetup[subfigure]{font=scriptsize,labelfont=scriptsize}


%\makeatletter
%\newcommand\titlegraphicii[1]{\def\inserttitlegraphicii{#1}}
%\titlegraphicii{}
%\setbeamertemplate{title page}
%{
%	\vbox{}
%	{\usebeamercolor[fg]{titlegraphic}\inserttitlegraphic\hfill\inserttitlegraphicii\par}
%	\begin{centering}
%		\begin{beamercolorbox}[sep=8pt,center]{institute}
%			\usebeamerfont{institute}\insertinstitute
%		\end{beamercolorbox}
%		\begin{beamercolorbox}[sep=8pt,center]{title}
%			\usebeamerfont{title}\inserttitle\par%
%			\ifx\insertsubtitle\@empty%
%			\else%
%			\vskip0.25em%
%			{\usebeamerfont{subtitle}\usebeamercolor[fg]{subtitle}\insertsubtitle\par}%
%			\fi%     
%		\end{beamercolorbox}%
%		\vskip1em\par
%		\begin{beamercolorbox}[sep=8pt,center]{date}
%			\usebeamerfont{date}\insertdate
%		\end{beamercolorbox}%\vskip0.5em
%		\begin{beamercolorbox}[sep=8pt,center]{author}
%			\usebeamerfont{author}\insertauthor
%		\end{beamercolorbox}
%	\end{centering}
%	%\vfill
%}
%
%\titlegraphic{\includegraphics[height=1.5cm,width=2cm]{unam.png}}
%\titlegraphicii{\includegraphics[height=1.5cm,width=2cm]{unam.png}}

%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------
\vspace*{3em}
\title[Ataques adversarios]{Ataques adversarios} % The short title appears at the bottom of every slide, the full title is only on the title page

\author[IIMAS, UNAM]{E. David Guzmán Ramírez} % Your name
\institute[] % Your institution as it will appear on the bottom of every slide, may be shorthand to save space
{	Licenciatura en Ciencia de Datos \\
	Introducción al Aprendizaje Profundo \\ \medskip M. en C. Berenice Montalvo Lezama \\ M. en C. Ricardo Montalvo Lezama
}
\date{{\tiny \today}} % Date, can be changed to a custom date

\vspace*{-5em}

\titlegraphic{\includegraphics[width=1.8cm]{Images/unam.png}\hspace*{7.5cm}~%
	\includegraphics[width=2.0cm]{Images/iimas.png} 
}

\begin{document}
	
	
\begin{frame}
	\titlepage % Print the title page as the first slide
\end{frame}

\begin{frame}
\frametitle{Contenidos}
\justify

\tableofcontents
\end{frame}

\section{Introducción}
\begin{frame}{Introducción}
\justify
\small
Las redes neuronales profundas son modelos poderosos que se han utilizado ampliamente para lograr un rendimiento cercano al nivel humano en una variedad de tareas. \medskip

Sin embargo, a pesar de su desempeño superior estudios recientes han encontrado que incluso los modelos del estado del arte son sumamente vulnerables a ataques adversarios.
\end{frame}

\begin{frame}{Introducción}
\justify
\small
Un ataque adversario es una muestra de datos de entrada que ha sido perturbada levemente con la intención de hacer fallar a un clasificador. 

\blfootnote{\vspace*{0.3cm} \href{https://arxiv.org/pdf/1412.6572.pdf}{Goodfellow et al., \emph{Explaining and harnessing adversarial examples}, 2016.}}

\begin{figure}[H]
\centering
\includegraphics[width=0.85\linewidth]{Images/ejemplo1_anotado.png}
\caption{Ejemplo de ataque adversario. Imagen tomada de \href{https://arxiv.org/pdf/1412.6572.pdf}{\emph{Explaining and harnessing adversarial examples}.}}
\end{figure}

\end{frame}

\begin{frame}{Introducción}
\justify
\small

\blfootnote{\vspace*{0.3cm} \href{https://arxiv.org/pdf/1412.6572.pdf}{Su et al., \emph{One Pixel Attack for Fooling Deep Neural Networks}, 2019.}}

\begin{figure}[H]
\centering
\includegraphics[width=0.3\linewidth]{Images/ejemplo3_anotado.png}
\caption{Ejemplo de ataque adversario en el que es posible engañar a la red cambiando únicamente un pixel. Imagen tomada de \href{https://arxiv.org/pdf/1710.08864.pdf}{\emph{One Pixel Attack for Fooling Deep Neural Networks}.}}
\end{figure}

\end{frame}

\begin{frame}{Introducción}
\justify
\small
Los escenarios de posibles ataques adversarios se pueden clasificar en diferentes maneras:

\begin{itemize}
\item \textbf{Ataque no dirigido:} el objetivo es hacer que el clasificador prediga una etiqueta incorrecta, la etiqueta incorrecta específica no importa.

\item \textbf{Ataque dirigido:} el objetivo es cambiar la predicción del clasificador a alguna clase objetivo específica.
\end{itemize}

\end{frame}

\begin{frame}{Introducción}
\justify
\small
En segundo lugar, los escenarios de ataque se pueden clasificar por la cantidad de conocimiento que el adversario tiene sobre el modelo:

\begin{itemize}
\item \textbf{Caja negra:} el atacante no sabe mucho sobre el modelo, pero puede sondear o consultar el modelo, es decir, darle algunas entradas y observar salidas.

\item \textbf{Caja blanca:} el atacante tiene pleno conocimiento del modelo, como la arquitectura del modelo y los valores de todos los parámetros y pesos entrenables.
\end{itemize}

\end{frame}


\section{Motivación}
\begin{frame}{Motivación}
\justify
\small
Los ataques adversarios plantean problemas de seguridad porque podrían usarse para realizar un ataque a los sistemas de aprendizaje profundo, incluso si el atacante no tiene acceso al modelo subyacente. \medskip

Con la introducción de modelos de aprendizaje profundo en cada vez más distintos aspectos de nuestra vida, los problemas que estos ataques adversarios pueden ocasionar son preocupantes.

\end{frame}

\begin{frame}{Motivación}
\justify
\small

\begin{figure}
\centering
\includegraphics[width=0.65\linewidth]{Images/ejemplo2_anotado.png}
\caption{Ataque adversario al sistema de navegación autónomo de un Tesla, en el que confunde un señalamiento de velocidad de 35 millas/hora por 85 millas/hora. Figura tomada de \href{https://www.technologyreview.com/2020/02/19/868188/hackers-can-trick-a-tesla-into-accelerating-by-50-miles-per-hour/}{MIT Technology Review: Trick a Tesla into accelerating by 50 miles per hour.}}
\end{figure}

\end{frame}

\section{Descripción del problema}
\begin{frame}{Descripción del problema}
\justify
\small
Para acelerar la investigación sobre ataques de adversarios, Google Brain organizó la  \emph{Competencia de Ataques y Defensas Adversarios} en la edición de 2017 de NIPS, la cual está disponible en \href{https://www.kaggle.com/c/nips-2017-non-targeted-adversarial-attack/overview}{Kaggle}, la competencia a su vez constaba de tres subcompeticiones:

\begin{itemize}
\item Ataques adversarios no dirigido.
\item Ataques adversarios dirigido.
\item Defensas contra ataques adversarios.
\end{itemize}

Por el momento me concentré en los ataques adversarios no dirigidos, la cual trata de un ataque de caja negra no dirigido, es decir, dada una imagen de entrada generar una imagen adversaria que engañe a un clasificador desconocido.

\end{frame}

\section{Análisis exploratorio}
\begin{frame}{Análisis exploratorio}
\justify	
\small

El dataset para esta competencia debía cumplir con 3 aspectos:

\begin{enumerate}
\item Conjunto de datos suficientemente grande y problema no trivial.
\item Problema bien conocido, por lo que las personas potencialmente pueden reutilizar los clasificadores existentes.
\item Muestras de datos que nunca se usaron antes.
\end{enumerate}

El conjunto de ImageNet cumple con los primeros dos requisitos, posteriormente se etiquetaron 1000 nuevas imágenes compatibles con ImageNet las cuales servían como el conjunto de datos de desarrollo para la competencia y para cumplir el tercer requisito. \medskip

Desafortunadamente, nunca hicieron público el dataset de evaluación ni el modelo de caja negra que había que engañar, por lo que no hay manera de comparar mis resultados con los de la competencia.

\end{frame}

\section{Propuesta de solución}
\begin{frame}{Propuesta de solución}
\justify	
\small

La idea es tratar de implementar varias técnicas de ataques adversarios\footnote{\vspace*{0.3cm} \href{https://reader.elsevier.com/reader/sd/pii/S209580991930503X?token=D051F74ED28FC5C7871D901893E9DAA76D35478F8C374211EC76DA66B1D71487CCB669164F543A5F3A04AE89239AC6C9&originRegion=us-east-1&originCreation=20210425231353}{Kui Ren et al., \emph{Adversarial Attacks and Defenses in Deep Learning}, 2020.}} sobre las arquitecturas existentes, particularmente usaré los siguientes cuatro métodos: 

\begin{itemize}
\item \textbf{FGSM (Fast Gradient Sign Method):} la idea es generar el ejemplo adversario $\bm{x}_{\text{adv}}$ de la siguiente forma
%
\[ \bm{x}_{\text{adv}} = \bm{x} + \epsilon \cdot \text{sign}\left( \nabla_{\bm{x}} J(\bm{\theta}, \bm{x}, y) \right). \]
%
donde $\epsilon$ es el orden de la perturbación, $J$ es la función de pérdida (usualmente cross-entropy), $\bm{\theta}$ los pesos del modelo, $\bm{x}$ la imagen original y $y$ la etiqueta.

\item \textbf{PGD (Projected Gradient Descent):} podemos verlo como una variante de varios pasos, donde $\alpha$ es la magnitud de la perturbación en cada paso
%
\[ \bm{x}_{\text{adv}}^{t+1} = \text{Proj}\left( \bm{x}_{\text{adv}}^{t} + \alpha \cdot  \text{sign}\left( \nabla_{\bm{x}} J(\bm{\theta}, \bm{x}, y) \right) \right) \]
\end{itemize}

\end{frame}

\begin{frame}{Propuesta de solución}
\justify	
\small

\begin{itemize}
\item \textbf{MIFGSM (Momentum Iterative Fast Gradient Sign Method):} algoritmo ganador de esta competencia, inspirados por los optimizadores con momento proponen
%
\[ \bm{x}_{\text{adv}}^{t+1} = \text{Clip}\left( \bm{x}_{\text{adv}}^{t} + \alpha \cdot  \text{sign}(g_{t+1}) \right), \]
%
donde $g_{t+1} = \xi \cdot g_{t} + \dfrac{ \nabla_{\bm{x}} J(\bm{\theta}, \bm{x}, y)}{\left\Vert J(\bm{\theta}, \bm{x}, y) \right\Vert} $, donde $\xi$ es un factor de decaimiento.

\item \textbf{One Pixel Attack:} plantea el problema como un problema de optimización
%
\begin{align*}
& \hspace{2em} \max_{\bm{x}_{\text{adv}}} f_{\text{adv}} (\bm{x} + \bm{x}_{\text{adv}}), \\
& \text{con la restricción } \left\Vert \bm{x}_{\text{adv}} \right\Vert \leq d,
\end{align*}
%
donde $d = 1$ para el caso de un ataque a un pixel. Particularmente proponen resolver este problema de optimización con \emph{evolución diferencial}.

\end{itemize}

\end{frame}

\begin{frame}{Propuesta de solución}
\justify	
\small

Hay una variedad de arquitecturas preentrenadas en \href{https://pytorch.org/vision/stable/models.html}{PyTorch} con el conjunto de datos de ImageNet que sirven a la perfección para esta tarea, particularmente usaré las siguientes

\begin{itemize}
\item AlexNet
\item Resnet18
\item Inception v3
\item MobileNet v2
\end{itemize}

Con estas arquitecturas y métodos se pueden explorar los ataques y defensas adversarias.

\end{frame}

\section{Resultados}
\begin{frame}{Resultados}
\justify	
\small
Con los ataques de caja blanca, donde los ejemplos adversarios se hacen a la medida de cada modelo, la disminución en el accuracy es enorme. Particularmente el ataque a un sólo pixel no es tan eficiente, pero en cualquier caso logra bajar el accuracy.
%
\begin{table}[H]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}cccccc@{}}
\toprule
\textbf{Modelo} & \textbf{\begin{tabular}[c]{@{}c@{}}Limpio \\ (acc@1/acc@5)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}FGSM \\ (acc@1/acc@5)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}PGD \\ (acc@1/acc@5)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}MIFGSM \\ (acc@1/acc@5)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}OnePixel \\ (acc@1/acc@5)\end{tabular}} \\ \midrule
AlexNet         & 60.9 / 84.6                                                              & 6.0 / 28.6                                                             & 2.9 / 19.3                                                            & 3.5 / 21.0                                                               & 58.3 / 83.4                                                                \\
ResNet-18       & 82.5 / 95.4                                                              & 3.5 / 24.3                                                             & 0.8 / 14.6                                                            & 1.0 / 13.5                                                               & 78.7 / 94.5                                                                \\
Inception v3    & 76.5 / 93.1                                                              & 10.8 / 39.3                                                            & 3.9 / 28.1                                                            & 5.1 / 27.6                                                               & 70.1 / 91.8                                                                \\
MobileNet v2    & 85.0 / 97.3                                                              & 3.5 / 24.4                                                             & 0.5 / 11.3                                                            & 0.6 / 8.8                                                                & 81.4 / 96.6                                                                \\ \bottomrule
\end{tabular}%
}
\caption{Ataques de caja blanca.}
\end{table}

\end{frame}

\begin{frame}{Resultados}
\justify	
\small

\begin{figure}[H]
\centering
\includegraphics[width=0.99\linewidth]{Images/hummingbird_alexnet_FGSM.png}
\caption{Ataque a AlexNet con FGSM.}
\end{figure}

\end{frame}

\begin{frame}{Resultados}
\justify	
\small

\begin{figure}[H]
\centering
\includegraphics[width=0.99\linewidth]{Images/inception_v3_OnePixel.png}
\caption{Ataque a Inception v3 con OnePixel.}
\end{figure}

\end{frame}

\begin{frame}{Resultados}
\justify	
\small
Para el ataque de caja negra generé los ejemplos adversarios con Inception v3 y los probé en MobileNet v2, el cual funcionaba como mi modelo de caja negra. En este caso fui más agresivo con los ataques. Aunque no es tan efectivo como un ataque de caja blanca, se logra bajar considerablemente el accuracy del modelo. 
%
\begin{table}[H]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}cccccc@{}}
\toprule
\textbf{Modelo} & \textbf{\begin{tabular}[c]{@{}c@{}}Limpio \\ (acc@1/acc@5)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}FGSM \\ (acc@1/acc@5)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}PGD \\ (acc@1/acc@5)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}MIFGSM \\ (acc@1/acc@5)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}OnePixel \\ (acc@1/acc@5)\end{tabular}} \\ \midrule
MobileNet v2    & 85.0 / 97.3                                                              & 51.7 / 78.3                                                            & 59.4 / 82.9                                                           & 56.3 / 81.5                                                              & 83.5 / 97.0                                                                \\ \bottomrule
\end{tabular}%
}
\caption{Ataque de caja negra a MobileNet v2.}
\end{table}

\end{frame}

\begin{frame}{Resultados}
\justify	
\small

\begin{figure}[H]
\centering
\includegraphics[width=0.99\linewidth]{Images/hummingbird_blackbox_FGSM.png}
\caption{Ataque de caja negra a MobileNet v2 con FGSM.}
\end{figure}

\end{frame}

\begin{frame}{Resultados}
\justify	
\small

\begin{figure}[H]
\centering
\includegraphics[width=0.99\linewidth]{Images/blackbox_OnePixel.png}
\caption{Ataque de caja negra a MobileNet v2 con OnePixel.}
\end{figure}

\end{frame}

\begin{frame}{Resultados}
\justify	
\small
Una de las formas de hacer más robustos los modelos a ataques adversarios es agregar ejemplos adversarios al conjunto de datos de entrenamiento. Para probar, agregué ejemplos adversarios al conjunto de CIFAR10 y entrené una versión modificada de MobileNet v2 sin y con los ejemplos adversarios.

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{graphicx}
\begin{table}[H]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}cccccc@{}}
\toprule
\textbf{Modelo}          & \textbf{\begin{tabular}[c]{@{}c@{}}Limpio \\ (accuracy)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}FGSM \\ (accuracy)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}PGD \\ (accuracy)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}MIFGSM \\ (accuracy)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}OnePixel \\ (accuracy)\end{tabular}} \\ \midrule
MobileNet v2             & 81.68                                                                 & 37.03                                                               & 28.29                                                              & 31.27                                                                 & 63.17                                                                   \\
MobileNet v2 adversarial & 80.62                                                                 & 77.44                                                               & 77.49                                                              & 77.35                                                                 & 77.58                                                                   \\ \bottomrule
\end{tabular}%
}
\caption{Entrenamiento sin y con ejemplos adversarios en CIFAR10 con una versión modificada de MobileNet v2.}
\end{table}

\end{frame}

\begin{frame}{Resultados}
\justify	
\small

\begin{figure}[H]
\centering
\includegraphics[width=0.7\linewidth]{Images/cifar10_FGSM.png}
\caption{Ataque adversario con FGSM a MobileNet v2 entrenada sin ejemplos adversarios.}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.7\linewidth]{Images/cifar10_adversarial_FGSM.png}
\caption{Ataque adversario con FGSM a MobileNet v2 entrenada con ejemplos adversarios.}
\end{figure}

\end{frame}

\section{Conclusiones}
\begin{frame}{Conclusiones}
\justify	
\small

\begin{itemize}
\item Los ataques adversarios son un fenómeno interesante y un problema importante en la seguridad del aprendizaje automático, por lo que es relevante hacer notar a la comunidad de este problema.

\item Ataques relativamente simples pueden engañar fácilmente incluso a los modelos más recientes sin que un humano llegue a notarlos.

\item El estudio de estos ataques a su vez nos puede ayudar a generar defensas para hacer a los modelos más robustos, confiables y seguros.
\end{itemize}

\end{frame}

%\section{Bibliografía}
%\begin{frame}{Bibliografía}
%\justify
%
%\footnotesize{
%
%\begin{thebibliography}{00}
%\bibitem{goodfellow-2014}
%\emph{\href{https://arxiv.org/pdf/1412.6572.pdf}{Link Prediction Using Supervised Learning}}, Mohammad Hasan, Vineet Chaoji, Saeed Salem, Mohammed Javeed Zaki, 2006.
%
%\bibitem{dblp-dataset}
%\emph{\href{https://dblp.org/faq/1474679.html}{Digital Bibliography \& Library Project (DBLP) dataset}},  Schloss Dagstuhl.
%\end{thebibliography}
%}
%
%\end{frame}

\end{document}
