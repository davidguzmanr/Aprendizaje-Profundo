@misc{FGSM,
	title={Explaining and Harnessing Adversarial Examples}, 
	author={Ian J. Goodfellow and Jonathon Shlens and Christian Szegedy},
	year={2015},
	eprint={1412.6572},
	archivePrefix={arXiv},
	primaryClass={stat.ML}
}

@misc{PGD,
	title={Towards Deep Learning Models Resistant to Adversarial Attacks}, 
	author={Aleksander Madry and Aleksandar Makelov and Ludwig Schmidt and Dimitris Tsipras and Adrian Vladu},
	year={2019},
	eprint={1706.06083},
	archivePrefix={arXiv},
	primaryClass={stat.ML}
}

@article{MIFGSM,
	author    = {Yinpeng Dong and
	Fangzhou Liao and
	Tianyu Pang and
	Xiaolin Hu and
	Jun Zhu},
	title     = {Discovering Adversarial Examples with Momentum},
	journal   = {CoRR},
	volume    = {abs/1710.06081},
	year      = {2017},
	url       = {http://arxiv.org/abs/1710.06081},
	archivePrefix = {arXiv},
	eprint    = {1710.06081},
	timestamp = {Thu, 24 Jan 2019 16:37:15 +0100},
	biburl    = {https://dblp.org/rec/journals/corr/abs-1710-06081.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{OnePixel,
  author    = {Jiawei Su and
               Danilo Vasconcellos Vargas and
               Kouichi Sakurai},
  title     = {One pixel attack for fooling deep neural networks},
  journal   = {CoRR},
  volume    = {abs/1710.08864},
  year      = {2017},
  url       = {http://arxiv.org/abs/1710.08864},
  archivePrefix = {arXiv},
  eprint    = {1710.08864},
  timestamp = {Mon, 13 Aug 2018 16:46:37 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1710-08864.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{NIPS-Competition,
	author    = {Alexey Kurakin and
	Ian J. Goodfellow and
	Samy Bengio and
	Yinpeng Dong and
	Fangzhou Liao and
	Ming Liang and
	Tianyu Pang and
	Jun Zhu and
	Xiaolin Hu and
	Cihang Xie and
	Jianyu Wang and
	Zhishuai Zhang and
	Zhou Ren and
	Alan L. Yuille and
	Sangxia Huang and
	Yao Zhao and
	Yuzhe Zhao and
	Zhonglin Han and
	Junjiajia Long and
	Yerkebulan Berdibekov and
	Takuya Akiba and
	Seiya Tokui and
	Motoki Abe},
	title     = {Adversarial Attacks and Defences Competition},
	journal   = {CoRR},
	volume    = {abs/1804.00097},
	year      = {2018},
	url       = {http://arxiv.org/abs/1804.00097},
	archivePrefix = {arXiv},
	eprint    = {1804.00097},
	timestamp = {Thu, 31 Oct 2019 16:31:22 +0100},
	biburl    = {https://dblp.org/rec/journals/corr/abs-1804-00097.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Summary-attacks,
	title = {Adversarial Attacks and Defenses in Deep Learning},
	journal = {Engineering},
	volume = {6},
	number = {3},
	pages = {346-360},
	year = {2020},
	issn = {2095-8099},
	doi = {https://doi.org/10.1016/j.eng.2019.12.012},
	url = {https://www.sciencedirect.com/science/article/pii/S209580991930503X},
	author = {Kui Ren and Tianhang Zheng and Zhan Qin and Xue Liu},
	keywords = {Machine learning, Deep neural network, Adversarial example, Adversarial attack, Adversarial defense},
	abstract = {With the rapid developments of artificial intelligence (AI) and deep learning (DL) techniques, it is critical to ensure the security and robustness of the deployed algorithms. Recently, the security vulnerability of DL algorithms to adversarial samples has been widely recognized. The fabricated samples can lead to various misbehaviors of the DL models while being perceived as benign by humans. Successful implementations of adversarial attacks in real physical-world scenarios further demonstrate their practicality. Hence, adversarial attack and defense techniques have attracted increasing attention from both machine learning and security communities and have become a hot research topic in recent years. In this paper, we first introduce the theoretical foundations, algorithms, and applications of adversarial attack techniques. We then describe a few research efforts on the defense techniques, which cover the broad frontier in the field. Several open problems and challenges are subsequently discussed, which we hope will provoke further research efforts in this critical area.}
}
